{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1bN2VVLoJ9DULyv8Xyf7zvlAxLWy5GuIG",
      "authorship_tag": "ABX9TyMt4PycQRUGJ7tPF1TYjpCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymsworks0712/myacademicprojects/blob/main/Human_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXteziZoAYcq",
        "outputId": "5b665ba2-6217-4422-85f3-7ccd8ee8f658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "6YsbsBDICmBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "# Update DATA_DIR to the correct path\n",
        "DATA_DIR = \"/content/drive/MyDrive/UCF-101\"\n",
        "IMG_SIZE = 64\n",
        "FRAMES_PER_VIDEO = 10\n",
        "CLASSES = [\"ApplyEyeMakeup\", \"Archery\", \"BabyCrawling\", \"Basketball\",\"PullUps\"]  # Subset of classes\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yEDmY49kCpFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load video frames\n",
        "def extract_frames(video_path, max_frames=FRAMES_PER_VIDEO):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or count >= max_frames:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    # Pad with zeros if fewer frames\n",
        "    while len(frames) < max_frames:\n",
        "        frames.append(np.zeros((IMG_SIZE, IMG_SIZE)))\n",
        "    return np.stack(frames)\n"
      ],
      "metadata": {
        "id": "31TLuBR_C6Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Custom Dataset\n",
        "class HARVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "        for cls in classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            for file in os.listdir(cls_dir)[:50]:\n",
        "                video_path = os.path.join(cls_dir, file)\n",
        "                frames = extract_frames(video_path)\n",
        "                self.data.append(frames)\n",
        "                self.labels.append(self.class_to_idx[cls])\n",
        "\n",
        "        self.data = np.array(self.data, dtype=np.float32) / 255.0\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.data[idx].reshape(-1)  # Flatten frames\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(video), torch.tensor(label)"
      ],
      "metadata": {
        "id": "2_GNbrChC_4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedHARNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(AdvancedHARNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.BatchNorm1d(hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "   def forward(self, x):\n",
        "        return self.net( x)\n"
      ],
      "metadata": {
        "id": "dhVv0ozTDEux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load data\n",
        "dataset = HARVideoDataset(DATA_DIR, CLASSES)\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cnIcrteUDIim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Initialize model\n",
        "input_size = IMG_SIZE * IMG_SIZE * FRAMES_PER_VIDEO  # Flattened video\n",
        "hidden_size = 512\n",
        "num_classes = len(CLASSES)\n",
        "\n",
        "model = AdvancedHARNet(input_size=40960, hidden_size=512, num_classes=len(CLASSES)).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "RdgKm1q1GiEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training\n",
        "print(\"Training...\")\n",
        "for epoch in range(70):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMyzG6CDGrk8",
        "outputId": "67493fe7-c9fb-41b2-f167-732ce0b7d260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Epoch 1, Loss: 0.1796\n",
            "Epoch 2, Loss: 0.0286\n",
            "Epoch 3, Loss: 0.0282\n",
            "Epoch 4, Loss: 0.0853\n",
            "Epoch 5, Loss: 0.0712\n",
            "Epoch 6, Loss: 0.0563\n",
            "Epoch 7, Loss: 0.0178\n",
            "Epoch 8, Loss: 0.0750\n",
            "Epoch 9, Loss: 0.2162\n",
            "Epoch 10, Loss: 0.0219\n",
            "Epoch 11, Loss: 0.1016\n",
            "Epoch 12, Loss: 0.0168\n",
            "Epoch 13, Loss: 0.0651\n",
            "Epoch 14, Loss: 0.1197\n",
            "Epoch 15, Loss: 0.0224\n",
            "Epoch 16, Loss: 0.0079\n",
            "Epoch 17, Loss: 0.1631\n",
            "Epoch 18, Loss: 0.0081\n",
            "Epoch 19, Loss: 0.0197\n",
            "Epoch 20, Loss: 0.0198\n",
            "Epoch 21, Loss: 0.4269\n",
            "Epoch 22, Loss: 0.0152\n",
            "Epoch 23, Loss: 0.1142\n",
            "Epoch 24, Loss: 0.2201\n",
            "Epoch 25, Loss: 0.0653\n",
            "Epoch 26, Loss: 0.0426\n",
            "Epoch 27, Loss: 0.0836\n",
            "Epoch 28, Loss: 0.0264\n",
            "Epoch 29, Loss: 0.0180\n",
            "Epoch 30, Loss: 0.0345\n",
            "Epoch 31, Loss: 0.0947\n",
            "Epoch 32, Loss: 0.0133\n",
            "Epoch 33, Loss: 0.0669\n",
            "Epoch 34, Loss: 0.0332\n",
            "Epoch 35, Loss: 0.1770\n",
            "Epoch 36, Loss: 0.0290\n",
            "Epoch 37, Loss: 0.0582\n",
            "Epoch 38, Loss: 0.0365\n",
            "Epoch 39, Loss: 0.0462\n",
            "Epoch 40, Loss: 0.2841\n",
            "Epoch 41, Loss: 0.0224\n",
            "Epoch 42, Loss: 0.1640\n",
            "Epoch 43, Loss: 0.0519\n",
            "Epoch 44, Loss: 0.0290\n",
            "Epoch 45, Loss: 0.1299\n",
            "Epoch 46, Loss: 0.0312\n",
            "Epoch 47, Loss: 0.0289\n",
            "Epoch 48, Loss: 0.0245\n",
            "Epoch 49, Loss: 0.0111\n",
            "Epoch 50, Loss: 0.0252\n",
            "Epoch 51, Loss: 0.0096\n",
            "Epoch 52, Loss: 0.0425\n",
            "Epoch 53, Loss: 0.0543\n",
            "Epoch 54, Loss: 0.0185\n",
            "Epoch 55, Loss: 0.0142\n",
            "Epoch 56, Loss: 0.0615\n",
            "Epoch 57, Loss: 0.0051\n",
            "Epoch 58, Loss: 0.1527\n",
            "Epoch 59, Loss: 0.0378\n",
            "Epoch 60, Loss: 0.0120\n",
            "Epoch 61, Loss: 0.0485\n",
            "Epoch 62, Loss: 0.0608\n",
            "Epoch 63, Loss: 0.0149\n",
            "Epoch 64, Loss: 0.1245\n",
            "Epoch 65, Loss: 0.0136\n",
            "Epoch 66, Loss: 0.0330\n",
            "Epoch 67, Loss: 0.0506\n",
            "Epoch 68, Loss: 0.0899\n",
            "Epoch 69, Loss: 0.0193\n",
            "Epoch 70, Loss: 0.0234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Testing\n",
        "print(\"\\nEvaluating...\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwreTMUvGySu",
        "outputId": "9de2b668-6336-4c8c-8333-8a9e08dc2262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating...\n",
            "Test Accuracy: 72.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_video_class(model, video_path, classes):\n",
        "    model.eval()  # Taking model to evaluation mode\n",
        "\n",
        "    # Finding Frame from Videos\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames / 255.0  # normalize\n",
        "    frames = torch.tensor(frames, dtype=torch.float32)\n",
        "\n",
        "    # Making frames 1D vector\n",
        "    input_tensor = frames.view(1, -1)  # shape: [1, 40960]\n",
        "    input_tensor = input_tensor.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted_class = torch.max(outputs, 1)\n",
        "\n",
        "    # Showing Class Name\n",
        "    print(\"ðŸŽ¬ Predicted Activity:\", classes[predicted_class.item()])"
      ],
      "metadata": {
        "id": "OKq6ANIbH5VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example : Giving a Videopath from Dataset\n",
        "sample_video_path = \"/content/drive/MyDrive/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c02.avi\"\n",
        "\n",
        "predict_video_class(model, sample_video_path, CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE0BWIrdICKR",
        "outputId": "b0fe3512-ef98-4b04-d5bf-5c5379a1baf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¬ Predicted Activity: ApplyEyeMakeup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EVfCOWQLq6fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pl735HsxWxhb"
      }
    }
  ]
}